<!DOCTYPE html>
<html>
<head>
  <title>Open iPhone Camera</title>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      height: 100%;
      width: 100%;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }
    #gif-source {
      display: none; /* hidden, but used for drawing */
    }
  </style>
</head>
<body>
  <video id="camera" width="720" height="560" autoplay playsinline></video>
  <canvas id="overlay"></canvas>
  <img id="gif-source" src="images/connect01.png" />

  <!-- FaceAPI + Gifler -->
  <script src="face-api.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gifler@0.1.0/gifler.min.js"></script>

  <script>
    const video = document.getElementById('camera');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const gif = document.getElementById('gif-source');
    
    //let gifCanvas = document.createElement('canvas');
    //let imageSize = 400;
    //gifCanvas.width = imageSize;
    //gifCanvas.height = imageSize;
    //const gifCtx = gifCanvas.getContext('2d');
    let noseX = 0, noseY = 0;

    async function startCamera() {
      navigator.mediaDevices.getUserMedia({ video: true })
        .then(stream => {
          video.srcObject = stream;
        })
        .catch(err => {
          alert('Camera access denied or unavailable: ' + err);
        });
    }

    async function loadModels() {
      const MODEL_URL = './models';
      await Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68TinyNet.loadFromUri(MODEL_URL),
      ]);
    }

    video.addEventListener('play', () => {
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(canvas, displaySize);

      setInterval(async () => {
        const detections = await faceapi.detectAllFaces(
          video,
          new faceapi.TinyFaceDetectorOptions()
        ).withFaceLandmarks(true);

        ctx.clearRect(0, 0, canvas.width, canvas.height);

        const resized = faceapi.resizeResults(detections, displaySize);
        resized.forEach(det => {
          const nose = det.landmarks.getNose();
          const noseTip = nose[3]; // Tip of the nose

          const size = 400; // smaller size for GIF
          ctx.drawImage(gif, noseTip.x - size / 2, noseTip.y - size / 2, size, size);
        });
      }, 20); // ~30 fps
    });

    loadModels().then(startCamera);
  </script>
</body>
</html>
